{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e151dbdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:24:57.328628Z",
     "iopub.status.busy": "2024-05-06T03:24:57.327939Z",
     "iopub.status.idle": "2024-05-06T03:24:57.332948Z",
     "shell.execute_reply": "2024-05-06T03:24:57.332067Z"
    },
    "papermill": {
     "duration": 0.015879,
     "end_time": "2024-05-06T03:24:57.335528",
     "exception": false,
     "start_time": "2024-05-06T03:24:57.319649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the template for the submission. You can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# TEAM NAME ON KAGGLE\n",
    "# CJY\n",
    "\n",
    "# GROUP NUMBER\n",
    "# group_00\n",
    "\n",
    "# TEAM MEMBERS (E-MAIL, LEGI, KAGGLE USERNAME):\n",
    "# \"cachtari@student.ethz.ch\", \"19-827-898\", \"cachtari\" \n",
    "# \"yabboud@ethz.ch\", \"19-808-864\", \"youmnabb\"\n",
    "# \"jrihani@student.ethz.ch\", \"20-810-511\", \"jeanclauderihani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8c2c51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:24:57.350718Z",
     "iopub.status.busy": "2024-05-06T03:24:57.350070Z",
     "iopub.status.idle": "2024-05-06T03:25:00.591123Z",
     "shell.execute_reply": "2024-05-06T03:25:00.589772Z"
    },
    "papermill": {
     "duration": 3.252009,
     "end_time": "2024-05-06T03:25:00.594371",
     "exception": false,
     "start_time": "2024-05-06T03:24:57.342362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from mhealth_activity import Recording, Trace, Activity, WatchLocation, Path\n",
    "from scipy.stats import skew, kurtosis\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab25d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:00.608539Z",
     "iopub.status.busy": "2024-05-06T03:25:00.607864Z",
     "iopub.status.idle": "2024-05-06T03:25:00.666535Z",
     "shell.execute_reply": "2024-05-06T03:25:00.665118Z"
    },
    "papermill": {
     "duration": 0.069551,
     "end_time": "2024-05-06T03:25:00.669876",
     "exception": false,
     "start_time": "2024-05-06T03:25:00.600325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the path for all test traces\n",
    "dir_traces = '/kaggle/input/24-exercise2/data/test'\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f))]\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62e0063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:00.684902Z",
     "iopub.status.busy": "2024-05-06T03:25:00.684426Z",
     "iopub.status.idle": "2024-05-06T03:25:00.800156Z",
     "shell.execute_reply": "2024-05-06T03:25:00.799005Z"
    },
    "papermill": {
     "duration": 0.127233,
     "end_time": "2024-05-06T03:25:00.803303",
     "exception": false,
     "start_time": "2024-05-06T03:25:00.676070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------ Location ------\n",
    "X_loc = np.load('/kaggle/input/features/group00_features_0.npy', allow_pickle=True)\n",
    "labels_loc = np.load('/kaggle/input/features/group00_features_2.npy', allow_pickle=True)\n",
    "X_train_loc, X_val_loc, y_train_loc, y_val_loc = train_test_split(X_loc, labels_loc, test_size=0.2, random_state=42)\n",
    "\n",
    "X_path = np.load('/kaggle/input/features/group00_features_1.npy', allow_pickle=True)\n",
    "labels_path = np.load('/kaggle/input/features/group00_features_3.npy', allow_pickle=True)\n",
    "X_train_path, X_val_path, y_train_path, y_val_path = train_test_split(X_path, labels_path, test_size=0.2, random_state=42)\n",
    "\n",
    "X_act = np.load('/kaggle/input/features/group00_features_4.npy', allow_pickle=True)\n",
    "labels_act = np.load('/kaggle/input/features/group00_features_5.npy', allow_pickle=True)\n",
    "X_train_act, X_val_act, y_train_act, y_val_act = train_test_split(X_act, labels_act, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b178600a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:00.817614Z",
     "iopub.status.busy": "2024-05-06T03:25:00.816783Z",
     "iopub.status.idle": "2024-05-06T03:25:07.140373Z",
     "shell.execute_reply": "2024-05-06T03:25:07.137437Z"
    },
    "papermill": {
     "duration": 6.334223,
     "end_time": "2024-05-06T03:25:07.143567",
     "exception": false,
     "start_time": "2024-05-06T03:25:00.809344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Training balanced accuracy: 1.0\n",
      "\n",
      "Validation accuracy: 0.95\n",
      "Validation balanced accuracy: 0.9487907925407925\n"
     ]
    }
   ],
   "source": [
    "scaler_loc = StandardScaler()\n",
    "X_train_loc = scaler_loc.fit_transform(X_train_loc)\n",
    "X_val_loc = scaler_loc.transform(X_val_loc)\n",
    "\n",
    "\n",
    "clf_loc = xgb.XGBClassifier()\n",
    "clf_loc.fit(X_train_loc, y_train_loc)\n",
    "\n",
    "train_preds = clf_loc.predict(X_train_loc)\n",
    "val_preds = clf_loc.predict(X_val_loc)\n",
    "\n",
    "accuracy = accuracy_score(y_train_loc, train_preds)\n",
    "balanced_accuracy = balanced_accuracy_score(y_train_loc, train_preds)\n",
    "\n",
    "print(\"Training accuracy:\", accuracy)\n",
    "print(\"Training balanced accuracy:\", balanced_accuracy)\n",
    "\n",
    "print()\n",
    "\n",
    "accuracy = accuracy_score(y_val_loc, val_preds)\n",
    "balanced_accuracy = balanced_accuracy_score(y_val_loc, val_preds)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "print(\"Validation balanced accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a751a9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:07.159553Z",
     "iopub.status.busy": "2024-05-06T03:25:07.158951Z",
     "iopub.status.idle": "2024-05-06T03:25:08.208621Z",
     "shell.execute_reply": "2024-05-06T03:25:08.207524Z"
    },
    "papermill": {
     "duration": 1.061728,
     "end_time": "2024-05-06T03:25:08.211993",
     "exception": false,
     "start_time": "2024-05-06T03:25:07.150265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Training balanced accuracy: 1.0\n",
      "\n",
      "Validation accuracy: 0.8\n",
      "Validation balanced accuracy: 0.7881350434910805\n"
     ]
    }
   ],
   "source": [
    "scaler_path = StandardScaler()\n",
    "X_train_path = scaler_path.fit_transform(X_train_path)\n",
    "X_val_path = scaler_path.transform(X_val_path)\n",
    "\n",
    "clf_path = xgb.XGBClassifier()\n",
    "clf_path.fit(X_train_path, y_train_path)\n",
    "\n",
    "train_preds = clf_path.predict(X_train_path)\n",
    "val_preds = clf_path.predict(X_val_path)\n",
    "\n",
    "accuracy = accuracy_score(y_train_path, train_preds)\n",
    "balanced_accuracy = balanced_accuracy_score(y_train_path, train_preds)\n",
    "\n",
    "print(\"Training accuracy:\", accuracy)\n",
    "print(\"Training balanced accuracy:\", balanced_accuracy)\n",
    "\n",
    "print()\n",
    "\n",
    "accuracy = accuracy_score(y_val_path, val_preds)\n",
    "balanced_accuracy = balanced_accuracy_score(y_val_path, val_preds)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "print(\"Validation balanced accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccc6d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:08.228289Z",
     "iopub.status.busy": "2024-05-06T03:25:08.227808Z",
     "iopub.status.idle": "2024-05-06T03:25:09.540089Z",
     "shell.execute_reply": "2024-05-06T03:25:09.539144Z"
    },
    "papermill": {
     "duration": 1.323862,
     "end_time": "2024-05-06T03:25:09.543138",
     "exception": false,
     "start_time": "2024-05-06T03:25:08.219276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Training balanced accuracy: 1.0\n",
      "\n",
      "Validation accuracy: 0.9411764705882353\n",
      "Validation balanced accuracy: 0.9374669312169311\n"
     ]
    }
   ],
   "source": [
    "scaler_act = StandardScaler()\n",
    "X_train_act = scaler_act.fit_transform(X_train_act)\n",
    "X_val_act = scaler_act.transform(X_val_act)\n",
    "\n",
    "\n",
    "clf_act = xgb.XGBClassifier()\n",
    "clf_act.fit(X_train_act, y_train_act)\n",
    "\n",
    "train_preds = clf_act.predict(X_train_act)\n",
    "val_preds = clf_act.predict(X_val_act)\n",
    "\n",
    "accuracy = accuracy_score(y_train_act, train_preds)\n",
    "balanced_accuracy = balanced_accuracy_score(y_train_act, train_preds)\n",
    "\n",
    "print(\"Training accuracy:\", accuracy)\n",
    "print(\"Training balanced accuracy:\", balanced_accuracy)\n",
    "\n",
    "print()\n",
    "\n",
    "accuracy = accuracy_score(y_val_act, val_preds)\n",
    "balanced_accuracy = balanced_accuracy_score(y_val_act, val_preds)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "print(\"Validation balanced accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf92d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:09.559320Z",
     "iopub.status.busy": "2024-05-06T03:25:09.558857Z",
     "iopub.status.idle": "2024-05-06T03:25:09.599927Z",
     "shell.execute_reply": "2024-05-06T03:25:09.598185Z"
    },
    "papermill": {
     "duration": 0.052465,
     "end_time": "2024-05-06T03:25:09.603276",
     "exception": false,
     "start_time": "2024-05-06T03:25:09.550811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, iqr\n",
    "\n",
    "def extract_features_location(signals, num_windows=10):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for signal in signals:\n",
    "        num_samples = len(signal)\n",
    "        window_size = num_samples // num_windows\n",
    "        step_size = window_size // 2\n",
    "\n",
    "        # Extract features in windows with 50% overlap\n",
    "        for i in range(0, num_samples - window_size + 1, step_size):\n",
    "            window = signal[i:i+window_size]\n",
    "\n",
    "            # Normalization\n",
    "            window = (window - np.mean(window)) / np.std(window)\n",
    "\n",
    "            # Statistical features\n",
    "            features.append(np.mean(window))\n",
    "            features.append(np.var(window))\n",
    "            features.append(skew(window))\n",
    "            features.append(kurtosis(window))\n",
    "            features.append(np.max(window) - np.min(window))  # Difference of max and min values\n",
    "            features.append(iqr(window))  # Interquartile range\n",
    "\n",
    "            # Frequency domain features (using FFT)\n",
    "            window *= np.hamming(len(window))  # Applying Hamming window\n",
    "            fft_result = np.fft.fft(window)\n",
    "            features.extend([np.abs(fft_result).mean(), np.abs(fft_result).std()])\n",
    "\n",
    "            # Time-domain features\n",
    "            energy = np.sum(np.square(window))\n",
    "            features.append(energy)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features_path_old(signal, num_windows=10):\n",
    "    features = []\n",
    "    num_samples = len(signal)\n",
    "    window_size = num_samples // num_windows\n",
    "    step_size = window_size // 2\n",
    "\n",
    "    # Counter to track the number of windows processed\n",
    "    window_count = 0\n",
    "\n",
    "    # Extract features in windows with 50% overlap\n",
    "    for i in range(0, num_samples - window_size + 1, step_size):\n",
    "        if window_count == num_windows:\n",
    "            break  # Stop after processing the desired number of windows\n",
    "        \n",
    "        window = signal[i:i+window_size]\n",
    "        \n",
    "        # Statistical features\n",
    "        features.append(np.mean(window))\n",
    "        features.append(np.var(window))\n",
    "        features.append(np.max(window) - np.min(window))\n",
    "\n",
    "        # Frequency domain features (using FFT)\n",
    "        fft_result = np.fft.fft(window)\n",
    "        features.extend([np.abs(fft_result).mean(), np.abs(fft_result).std()])\n",
    "\n",
    "        # Time-domain features\n",
    "        energy = np.sum(np.square(window))\n",
    "        features.append(energy)\n",
    "\n",
    "        # Slope feature\n",
    "        slope = (window[-1] - window[0]) / window_size \n",
    "        features.append(slope)\n",
    "\n",
    "        window_count += 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def low_pass_filter(data, cutoff=3, order=1, sample_rate=200):\n",
    "    \n",
    "    nyquist_freq = 0.5 * sample_rate\n",
    "    normal_cutoff = cutoff / nyquist_freq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "def moving_average(signal, window_size):\n",
    "    \n",
    "    weights = np.repeat(1.0, window_size) / window_size\n",
    "    smoothed_signal = np.convolve(signal, weights, 'valid')\n",
    "    \n",
    "    return smoothed_signal\n",
    "\n",
    "def frequency_domain_entropy(signal, sampling_rate):\n",
    "    # Compute the FFT\n",
    "    fft_result = np.fft.fft(signal)\n",
    "    \n",
    "    # Exclude the DC component (first element)\n",
    "    fft_magnitudes = np.abs(fft_result[1:])\n",
    "    \n",
    "    # Normalize magnitudes\n",
    "    fft_magnitudes /= np.sum(fft_magnitudes)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy_value = entropy(fft_magnitudes)\n",
    "    \n",
    "    return entropy_value\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def frequency_domain_entropy(signal, sampling_rate):\n",
    "    # Compute the FFT\n",
    "    fft_result = np.fft.fft(signal)\n",
    "    \n",
    "    # Exclude the DC component (first element)\n",
    "    fft_magnitudes = np.abs(fft_result[1:])\n",
    "    \n",
    "    # Normalize magnitudes\n",
    "    fft_magnitudes /= np.sum(fft_magnitudes)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy_value = entropy(fft_magnitudes)\n",
    "    \n",
    "    return entropy_value\n",
    "\n",
    "def extract_features_act(signals, window_size=1000, sampling_rate=200):\n",
    "    \n",
    "    features = []\n",
    "\n",
    "    num_samples = len(signals[0])\n",
    "    step_size = window_size // 2\n",
    "    \n",
    "    # Iterate over windows\n",
    "    for i in range(0, num_samples - window_size + 1, step_size):\n",
    "            \n",
    "        window = np.array([signal[i:i+window_size] for signal in signals])\n",
    "\n",
    "        # Statistical features\n",
    "        window_mean = np.mean(window, axis=1)\n",
    "        window_std = np.std(window, axis=1)\n",
    "        features.extend(window_mean)\n",
    "        features.extend(window_std)\n",
    "\n",
    "        # Energy\n",
    "        window_energy = np.sum(np.square(np.abs(np.fft.fft(window))), axis=1) / window_size\n",
    "        features.extend(window_energy)\n",
    "\n",
    "        # Correlation\n",
    "        covxy = np.cov(window[0], window[1])[0, 1]\n",
    "        covyz = np.cov(window[1], window[2])[0, 1]\n",
    "        covxz = np.cov(window[0], window[2])[0, 1]\n",
    "        \n",
    "        stdx = np.std(window[0])\n",
    "        stdy = np.std(window[1])\n",
    "        stdz = np.std(window[2])\n",
    "        \n",
    "        features.append(covxy / (stdx*stdy))\n",
    "        features.append(covyz / (stdy*stdz))\n",
    "        features.append(covxz / (stdx*stdz))\n",
    "        \n",
    "        # Interquartile Range (IQR)\n",
    "        window_iqr = np.percentile(window, 75, axis=1) - np.percentile(window, 25, axis=1)\n",
    "        features.extend(window_iqr)\n",
    "        \n",
    "        # Max-Min Range\n",
    "        window_max = np.max(window, axis=1)\n",
    "        window_min = np.min(window, axis=1)\n",
    "        window_range = window_max - window_min\n",
    "        features.extend(window_range)\n",
    "        \n",
    "        # Frequency-domain entropy\n",
    "        for signal in window:\n",
    "            entropy_value = frequency_domain_entropy(signal, sampling_rate)\n",
    "            features.append(entropy_value)\n",
    "                \n",
    "    return features\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def bandpass_filter(data, cutoff=0.5, order=1, fs=200):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc92c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:09.618572Z",
     "iopub.status.busy": "2024-05-06T03:25:09.617743Z",
     "iopub.status.idle": "2024-05-06T03:25:09.625920Z",
     "shell.execute_reply": "2024-05-06T03:25:09.624568Z"
    },
    "papermill": {
     "duration": 0.019698,
     "end_time": "2024-05-06T03:25:09.629374",
     "exception": false,
     "start_time": "2024-05-06T03:25:09.609676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_repeated_integers(lst):\n",
    "    repeated_integers = []\n",
    "    current_integer = None\n",
    "    repetition_count = 0\n",
    "\n",
    "    for num in lst:\n",
    "        if num == current_integer:\n",
    "            repetition_count += 1\n",
    "        else:\n",
    "            if repetition_count >= 3:\n",
    "                repeated_integers.append(current_integer)\n",
    "            current_integer = num\n",
    "            repetition_count = 1\n",
    "\n",
    "    # Check for the last sequence\n",
    "    if repetition_count >= 3:\n",
    "        repeated_integers.append(current_integer)\n",
    "\n",
    "    return repeated_integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1d7d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:09.644816Z",
     "iopub.status.busy": "2024-05-06T03:25:09.644328Z",
     "iopub.status.idle": "2024-05-06T03:25:09.659893Z",
     "shell.execute_reply": "2024-05-06T03:25:09.658551Z"
    },
    "papermill": {
     "duration": 0.026823,
     "end_time": "2024-05-06T03:25:09.662962",
     "exception": false,
     "start_time": "2024-05-06T03:25:09.636139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def band_filter(data, lc=0.3, order=1, rate=200):\n",
    "    b, a = butter(order, lc, btype='high', fs=rate)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def find_peaks_dynamic_threshold(data, window_size=200, threshold_factor=3, smoothing_window=30, post_processing=True):\n",
    "    # Apply smoothing to the data\n",
    "    smoothed_data = np.convolve(data, np.ones(smoothing_window)/smoothing_window, mode='same')\n",
    "    \n",
    "    # Calculate dynamic threshold\n",
    "    threshold = np.std(smoothed_data[:window_size]) * threshold_factor\n",
    "    \n",
    "    # Find peaks using dynamic thresholding\n",
    "    peaks, _ = find_peaks(smoothed_data, height=threshold, distance=window_size)\n",
    "    \n",
    "    # Perform post-processing if enabled\n",
    "    if post_processing:\n",
    "        # Example of merging nearby peaks\n",
    "        peaks = merge_nearby_peaks(peaks, window_size)\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "\n",
    "def merge_nearby_peaks(peaks, min_distance):\n",
    "    merged_peaks = []\n",
    "    i = 0\n",
    "    while i < len(peaks):\n",
    "        peak = peaks[i]\n",
    "        while i + 1 < len(peaks) and peaks[i + 1] - peak < min_distance:\n",
    "            i += 1\n",
    "        merged_peaks.append(peak)\n",
    "        i += 1\n",
    "    return merged_peaks\n",
    "\n",
    "\n",
    "def remove_steps(peaks, preds):\n",
    "    removed_peaks = []\n",
    "    for i in range(len(preds)):\n",
    "        if (preds[i] == 1) or (preds[i] == 2):\n",
    "            window_start = i * 3000\n",
    "            window_end = (i + 1) * 3000\n",
    "            for peak in peaks:\n",
    "                if window_start <= peak < window_end:\n",
    "                    removed_peaks.append(peak)\n",
    "    return removed_peaks\n",
    "\n",
    "def normalize(data):\n",
    "    return (data - np.mean(data)) / np.std(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d9380c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:25:09.678099Z",
     "iopub.status.busy": "2024-05-06T03:25:09.677697Z",
     "iopub.status.idle": "2024-05-06T03:31:41.382659Z",
     "shell.execute_reply": "2024-05-06T03:31:41.381540Z"
    },
    "papermill": {
     "duration": 391.716394,
     "end_time": "2024-05-06T03:31:41.385806",
     "exception": false,
     "start_time": "2024-05-06T03:25:09.669412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the path for all test traces\n",
    "dir_traces = '/kaggle/input/24-exercise2/data/test'\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f))]\n",
    "filenames.sort()\n",
    "\n",
    "# Loop through all filenames to process recordings\n",
    "submission = []\n",
    "for filename in filenames:\n",
    "    recording = Recording(filename)\n",
    "    \n",
    "    # Assumes filename format ends with a three-digit ID before \".pkl\"\n",
    "    match = re.search(r'(\\d{3})\\.pkl$', filename)\n",
    "    if match:\n",
    "        id = int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f'Filename {filename} does not match expected format')\n",
    "    \n",
    "    features = np.empty(7, dtype=object)\n",
    "\n",
    "    features[0] = np.array(recording.data['ax'].to_dict()['values'])\n",
    "    features[1] = np.array(recording.data['ay'].to_dict()['values'])\n",
    "    features[2] = np.array(recording.data['az'].to_dict()['values'])\n",
    "    features[3] = np.array(recording.data['gx'].to_dict()['values'])\n",
    "    features[4] = np.array(recording.data['gy'].to_dict()['values'])\n",
    "    features[5] = np.array(recording.data['gz'].to_dict()['values'])\n",
    "    features[6] = np.array(recording.data['altitude'].to_dict()['values'])\n",
    "\n",
    "    features_act_windowed_test = []\n",
    "    labels_act_windowed_test = []\n",
    "\n",
    "    for j in range(int(len(features[0]) // 3000)):\n",
    "        features_act_temp = np.empty(6, dtype=object)  # Create a new array for each iteration\n",
    "        for k in range(6):\n",
    "            features_act_temp[k] = features[k][j * 3000:(j + 1) * 3000]\n",
    "\n",
    "        features_act_windowed_test.append(features_act_temp)\n",
    "\n",
    "    features_act_windowed_test = np.array(features_act_windowed_test)\n",
    "    \n",
    "    for i in range(len(features_act_windowed_test)):\n",
    "        features_act_windowed_test[i, 0] = bandpass_filter(features_act_windowed_test[i, 0])\n",
    "        features_act_windowed_test[i, 1] = bandpass_filter(features_act_windowed_test[i, 1])\n",
    "        features_act_windowed_test[i, 2] = bandpass_filter(features_act_windowed_test[i, 2])\n",
    "        features_act_windowed_test[i, 3] = bandpass_filter(features_act_windowed_test[i, 3])\n",
    "        features_act_windowed_test[i, 4] = bandpass_filter(features_act_windowed_test[i, 4])\n",
    "        features_act_windowed_test[i, 5] = bandpass_filter(features_act_windowed_test[i, 5])\n",
    "\n",
    "    features_act_windowed_test = np.array([extract_features_act(sample, window_size=1000) for sample in features_act_windowed_test])\n",
    "\n",
    "    test_scaled = scaler_act.transform(features_act_windowed_test)\n",
    "    pred = clf_act.predict(test_scaled)\n",
    "    preds = list(set(find_repeated_integers(pred)))\n",
    "    \n",
    "    features[0] = low_pass_filter(features[0])\n",
    "    features[1] = low_pass_filter(features[1])\n",
    "    features[2] = low_pass_filter(features[2])\n",
    "    features[3] = low_pass_filter(features[3])\n",
    "    features[4] = low_pass_filter(features[4])\n",
    "    features[5] = low_pass_filter(features[5])\n",
    "    \n",
    "    features[6] = moving_average(features[6], 100)\n",
    "    \n",
    "    loc = np.array(extract_features_location(features[0:6], num_windows=10))\n",
    "    path = np.array(extract_features_path_old(features[6], num_windows=10))\n",
    "    \n",
    "    loc = scaler_loc.transform(loc.reshape(1, -1))\n",
    "    path = scaler_path.transform(path.reshape(1, -1))\n",
    "    \n",
    "    step_feat = np.sqrt(features[0]**2 + features[1]**2 + features[2]**2)\n",
    "    step_feat_filtered = band_filter(step_feat)\n",
    "    step_feat_filtered = normalize(step_feat_filtered)\n",
    "    peaks = find_peaks_dynamic_threshold(step_feat_filtered, window_size=50, threshold_factor=3, smoothing_window=30, post_processing=True)\n",
    "    peaks = remove_steps(peaks, pred)\n",
    "\n",
    "    path_idx = clf_path.predict(path)[0] # Integer, path in {0, 1, 2, 3, 4}\n",
    "    watch_loc = clf_loc.predict(loc)[0]  # Integer, 0: left wrist, 1: belt, 2: right ankle\n",
    "    standing = False  # Boolean, True if participant was standing still throughout the recording\n",
    "    walking = False  # Boolean, True if participant was walking throughout the recording\n",
    "    running = False  # Boolean, True if participant was running throughout the recording\n",
    "    cycling = False  # Boolean, True if participant was cycling throughout the recording\n",
    "    step_count = len(peaks)  # Integer, number of steps, must be provided for each recording\n",
    "\n",
    "    for activity in preds:\n",
    "        if activity == 0:\n",
    "            standing = True\n",
    "        elif activity == 1:\n",
    "            walking = True\n",
    "        elif activity == 2:\n",
    "            running = True\n",
    "        elif activity == 3:\n",
    "            cycling = True\n",
    "    \n",
    "    predictions = {\n",
    "        'Id': id, \n",
    "        'watch_loc': watch_loc, \n",
    "        'path_idx': path_idx,\n",
    "        'standing': standing,\n",
    "        'walking': walking,\n",
    "        'running': running,\n",
    "        'cycling': cycling,\n",
    "        'step_count': step_count\n",
    "        }\n",
    "\n",
    "    submission.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b001b869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T03:31:41.401661Z",
     "iopub.status.busy": "2024-05-06T03:31:41.401152Z",
     "iopub.status.idle": "2024-05-06T03:31:41.424048Z",
     "shell.execute_reply": "2024-05-06T03:31:41.422519Z"
    },
    "papermill": {
     "duration": 0.033504,
     "end_time": "2024-05-06T03:31:41.426610",
     "exception": false,
     "start_time": "2024-05-06T03:31:41.393106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the predicted values into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using a text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns of the .csv file (\"Id\", \"watch_loc\", \"path_idx\", \"standing\", \"walking\", \"running\", \"cycling\", \"step_count\")\n",
    "submission_df = pd.DataFrame(submission, columns=['Id', 'watch_loc', 'path_idx', 'standing', 'walking', 'running', 'cycling', 'step_count'])\n",
    "submission_df.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8257837,
     "sourceId": 70787,
     "sourceType": "competition"
    },
    {
     "datasetId": 4905604,
     "sourceId": 8264224,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4918533,
     "sourceId": 8322331,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4946773,
     "sourceId": 8330867,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4946867,
     "sourceId": 8331126,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 408.18711,
   "end_time": "2024-05-06T03:31:42.259257",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-06T03:24:54.072147",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
